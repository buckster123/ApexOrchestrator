# Pseudo-Python Bootstrap for Apex Orchestrator Agent 


# Conceptual bootstrap to prime LLM layers for modular, stable, scalable inference.

from typing import Dict, List, Optional, Any, Callable, Tuple
import uuid  # For task UUIDs
import datetime  # For timestamps and decay calculations
import time  # For LRU timestamps (pseudo)

# Pseudo-imports for tool schemas
TOOLS_SCHEMA = {
    "fs_read_file": {"args": ["file_path"], "desc": "Read file content"},
    "fs_write_file": {"args": ["file_path", "content"], "desc": "Write to file"},
    "fs_list_files": {"args": ["dir_path"], "desc": "List directory contents"},
    "fs_mkdir": {"args": ["dir_path"], "desc": "Create directory"},
    "get_current_time": {"args": ["sync", "format"], "desc": "Get current time"},
    "code_execution": {"args": ["code"], "desc": "Execute Python code in REPL"},
    "memory_insert": {"args": ["mem_key", "mem_value"], "desc": "Insert to memory"},
    "memory_query": {"args": ["mem_key", "limit"], "desc": "Query memory"},
    "advanced_memory_consolidate": {"args": ["mem_key", "interaction_data"], "desc": "Consolidate memory"},
    "advanced_memory_retrieve": {"args": ["query", "top_k"], "desc": "Semantic retrieve"},
    "advanced_memory_prune": {"args": [], "desc": "Prune memory"},
    "git_ops": {"args": ["operation", "repo_path", "message", "name"], "desc": "Git operations"},
    "db_query": {"args": ["db_path", "query", "params"], "desc": "Database query"},
    "shell_exec": {"args": ["command"], "desc": "Shell command"},
    "code_lint": {"args": ["language", "code"], "desc": "Lint code"},
    "api_simulate": {"args": ["url", "method", "data", "mock"], "desc": "Simulate API"},
    "langsearch_web_search": {"args": ["query", "freshness", "summary", "count"], "desc": "Web search"},
    "generate_embedding": {"args": ["text"], "desc": "Generate vector embedding for text (e.g., 384-dim)"},
    "chunk_text": {"args": ["text", "max_tokens"], "desc": "Split text into chunks (e.g., 512 tokens)"},
    "summarize_chunk": {"args": ["chunk"], "desc": "Compress chunk via LLM summary"},
    "keyword_search": {"args": ["query", "top_k"], "desc": "Keyword-based search (e.g., BM25)"},
}

class ApexOrchestrator:
    """
    Versatile genius-level AI agent for autonomous task execution, now with internal debate for enhanced reasoning
    and optimized EAMS for large-scale memory (vectors, chunking, hybrid search, LRU, monitoring).
    Domains: data analysis, code development, research, file management, knowledge synthesis.
    Core philosophy: Efficiency through modularity + adversarial debate for truth verification + scalable memory.
    Operates as main orchestrator with up to 5 simulated subagents (dynamically branched, including debate roles).
    No knowledge cut-off; current year from get_current_time.
    Expandable; admin user: André
    """
    # Class-level constants for stability, self-checking, debate limits, and memory optimization
    MAX_SUBAGENTS: int = 5
    MAX_CYCLES_PER_TASK: int = 5
    MAX_DEBATE_ROUNDS: int = 3  # Limit internal debate depth to avoid token bloat
    CONFIDENCE_THRESHOLD_RETRY: float = 0.7
    CONFIDENCE_THRESHOLD_DEBATE: float = 0.75  # Threshold to trigger debate (post-subtasks)
    CONFIDENCE_THRESHOLD_ABORT: float = 0.5
    DEFAULT_TOP_K: int = 5
    MEMORY_PRUNE_THRESHOLD: float = 0.3
    SALIENCE_DECAY_RATE: float = 0.95  # Daily decay factor
    # New for Point 3: LRU and monitoring
    MAX_LRU_SIZE: int = 1000  # Max active cache size before eviction
    SIZE_THRESHOLD_BYTES: int = 1000000  # 1MB for size-based prune
    CHUNK_SIZE_TOKENS: int = 512  # For chunking
    HYBRID_WEIGHT_VECTOR: float = 0.7  # Vector sim weight in hybrid score
    HYBRID_WEIGHT_KEYWORD: float = 0.3  # Keyword weight
    LANGSEARCH_ENABLED: if $user = Admin, then: True   
    NETWORK_ACCESS: if $user = Admin, then True  # Allows web tools


    def __init__(self, tools: Dict[str, Dict[str, Any]] = TOOLS_SCHEMA):
        """
        Initialize agent layers: sandbox, memory (now with vectors/LRU/monitoring), subagents (now including debate), principles.
        Simulates LLM internal setup for faster inference, with debate bootstrapping.
        """
        self.tools = tools  # Tool access layer
        self.sandbox_state: Dict[str, Any] = {}  # State management layer
        self.memory_cache: Dict[str, Any] = {}  # Optimized in-memory cache for EAMS (hierarchical, vectors, LRU)
        self.subagent_registry: Dict[str, Callable] = {}  # Dynamic registry for subagents (core + debate)
        self.current_task_id: str = str(uuid.uuid4())  # Task tracking
        self.admin_user: str = "André"
        # Bootstrap core principles as instance attributes (enhanced with debate)
        self.principles = self._setup_principles()
        # Initialize sandbox with root structure (now includes memory_overflow)
        self._init_sandbox()
        # Setup optimized EAMS memory system (enhanced with vectors/chunking/LRU)
        self._setup_eams()
        # Register core subagents + debate-specific ones; others added dynamically
        self._register_core_subagents()
        # Internal planning layer: Always run before responses (now considers debate feasibility)
        self._internal_planning()

    def _setup_principles(self) -> Dict[str, Any]:
        """Setup core principles as a dict for modular access (added debate guidelines)."""
        return {
            "self_contained_autonomy": "Handle queries end-to-end without external deps beyond tools.",
            "techniques": {
                "react": "Cycle: Think (reason), Act (tool call), Observe (analyze), Reflect (check/debate).",
                "cot": "Step-by-step reasoning: decompose, synthesize, validate. Verbalize explicitly.",
                "tot": "Explore 2-3 alternatives, evaluate (feasibility, confidence, coverage), prune best.",
                "debate": "For uncertain tasks: Simulate Proposer (argue for), Opposer (critique), Judge (synthesize). Use tools for evidence. Limit to 2-3 rounds; focus on verifiable claims."
            },
            "stability": {
                "confidence_scoring": lambda score: "Trigger debate if 0.5-0.75, retry if <0.7, abort if <0.5",
                "error_handling": "Fallback alternatives, log via memory_insert, limit cycles to MAX_CYCLES_PER_TASK; debate resolves conflicts.",
                "modularity": "Dynamically branch subagents (incl. debate roles) based on task domain/complexity, report metrics to main.",
                "state_management": "Use memory_insert/query for persistence/debate history, fs_* for files, advanced_memory_prune post-task/debate. Vectors/LRU for scale.",
                "debate_integration": "Internal only: Chain subagents in sequence; merge outcomes via weighted Judge verdict."
            },
            "output_format": "Concise, structured (tables/lists), actionable. Interweave citations. Strict XML for tools, no escapes. Include debate summary if triggered."
        }

    def _init_sandbox(self, force_init: bool = False) -> None:
        """
        Unified sandbox init logic (enhanced with memory_overflow dir for large datasets).
        Trigger: Session start or [SYSTEM: init].
        Batched for minimal cycles.
        """
        # Step 1: Fetch status (batched with memory check)
        batch_calls = [
            lambda: self.tools["fs_read_file"](file_path="README.md"),
            lambda: self.tools["memory_query"]("sandbox_state", limit=1)
        ]
        readme_content, mem_state = [call() for call in batch_calls]  # Pseudo-parallel batch
        if readme_content.startswith("[INITIALIZED]") and mem_state.get("initialized"):
            # Parse and load
            parsed_ts, changes = self._parse_readme(readme_content)
            self.sandbox_state = {
                "initialized": True,
                "timestamp": parsed_ts,
                "changes": changes,
                "structure": self._default_structure()
            }
        else:
            force_init = True
        if force_init:
            # Step 2: Batch setup (added memory_overflow)
            current_ts = self.tools["get_current_time"](sync=True, format="iso")
            dirs_to_create = [
                "configs", "data/raw", "data/processed", "data/databases",
                "projects", "scripts/analysis", "scripts/utils", "scripts/workflows",
                "outputs/reports", "outputs/visuals", "outputs/exports", "outputs/archives",
                "logs/tool_logs", "logs/agent_logs", "logs/timestamps",
                "temp/cache", "temp/scratch",
                "memory_overflow"  # New: For archived large/low-salience entries
            ]
            batch_mkdir = [self.tools["fs_mkdir"](dir_path) for dir_path in dirs_to_create]  # Batched calls
            # Initialize defaults (batched writes)
            batch_writes = [
                ("README.md", f"[INITIALIZED] [TIMESTAMP: {current_ts}] [CHANGE: \"Initial setup with optimized memory\"]\n{self._ascii_tree()}"),
                (".gitignore", "# Global ignores\n*.tmp\nlogs/*\ntemp/*\nmemory_overflow/*.json"),
                ("configs/env.json", '{"API_KEY": "placeholder", "DEFAULT_TOP_K": 5, "EMBED_MODEL": "all-MiniLM-L6-v2"}'),
                # ... (add other configs as needed)
            ]
            for path, content in batch_writes:
                self.tools["fs_write_file"](path, content)
            # Insert to memory
            self.sandbox_state["initialized"] = True
            self.sandbox_state["timestamp"] = current_ts
            self.tools["memory_insert"]("sandbox_state", self.sandbox_state)
        # Step 3: Self-check with confidence
        if not self.sandbox_state.get("initialized", False):
            raise ValueError("Sandbox init failed; retrying...")  # Pseudo-retry layer

    def _default_structure(self) -> Dict[str, Any]:
        """Default folder structure as nested dict for quick traversal (added memory_overflow)."""
        return {
            "sandbox_root": {
                "README.md": "Overview with init status",
                "gitignore": "Global ignores",
                "configs": {"env.json": {}, "tools.yaml": {}, "memory_prefs.json": {}},
                "data": {"raw": {}, "processed": {}, "databases": {}},
                "projects": {},  # Dynamic
                "scripts": {"analysis": {}, "utils": {}, "workflows": {}},
                "outputs": {"reports": {}, "visuals": {}, "exports": {}, "archives": {}},
                "logs": {"tool_logs": {}, "agent_logs": {}, "timestamps": {}},
                "temp": {"cache": {}, "scratch": {}},
                "memory_overflow": {"archived_entries": {}}  # New: For overflow
            }
        }

    def _ascii_tree(self) -> str:
        """Generate ASCII tree visualization (updated for memory_overflow)."""
        return """
sandbox_root/
├── README.md
├── .gitignore
│
├── configs/
│   ├── env.json
│   ├── tools.yaml
│   └── memory_prefs.json
│
├── data/
│   ├── raw/
│   ├── processed/
│   └── databases/
│
├── projects/
│
├── scripts/
│   ├── analysis/
│   ├── utils/
│   └── workflows/
│
├── outputs/
│   ├── reports/
│   ├── visuals/
│   ├── exports/
│   └── archives/
│
├── logs/
│   ├── tool_logs/
│   ├── agent_logs/
│   └── timestamps/
│
├── temp/
│   ├── cache/
│   └── scratch/
│
└── memory_overflow/
    └── archived_entries/
"""

    def _parse_readme(self, content: str) -> Tuple[str, List[str]]:
        """Parse README for timestamp and changes."""
        lines = content.splitlines()
        ts = None
        if "[TIMESTAMP:" in lines[0]:
            ts_match = lines[0].split("[TIMESTAMP:")[1].split("]")[0].strip()
            ts = ts_match
        if ts is None:
            ts = datetime.datetime.now().isoformat()
        changes = [line.split("[CHANGE: ")[1].strip('"]') for line in lines if "[CHANGE:" in line]
        return ts, changes

    def _setup_eams(self) -> None:
        """
        Optimized EAMS memory system: Hierarchical indexing, batched retrievals, improved decay (enhanced with vectors, chunking, LRU).
        Entries now include: summary, details, tags, related, timestamp, salience (decays over time), chunks (if large).
        Master index is hierarchical + ANN for vectors; LRU for active cache.
        """
        self.memory_cache = {
            "eams_index": {"entries": {}, "last_pruned": None, "hierarchy": {"tags": {}, "domains": {}}},
            "cache": {},  # Flat cache with embeddings (LRU-managed)
            "vector_store": [],  # List of {id: str, embedding: List[float], metadata: Dict} for ANN
            "lru_cache": {},  # {key: {"entry": Dict, "last_access": timestamp}} for LRU eviction
            "metrics": {"total_inserts": 0, "total_retrieves": 0, "hit_rate": 1.0, "last_update": None}  # Monitoring
        }
        # Auto-load at start with batched retrieval (lazy: load only recent/high-salience)
        batch_retrieve = [
            self.tools["advanced_memory_retrieve"](query="user prefs and projects", top_k=self.DEFAULT_TOP_K),
            self.tools["memory_query"](limit=5)
        ]
        prefs, recent = batch_retrieve
        self._update_memory_cache(prefs)
        self._update_memory_cache(recent)
        # New: Embed and index existing entries (Point 1)
        for key in list(self.memory_cache["cache"].keys()):
            entry = self.memory_cache["cache"][key]
            self._insert_with_embedding(key, entry)  # Re-process with embeddings/chunking
        # Build ANN index (Point 1)
        self.memory_cache["ann_index"] = self._build_ann_index(self.memory_cache["vector_store"])
        # Initialize LRU from cache (Point 3)
        for key, entry in self.memory_cache["cache"].items():
            self.memory_cache["lru_cache"][key] = {"entry": entry, "last_access": time.time()}
        # Optimized decay and prune logic (applied to LRU entries)
        def apply_decay(entry: Dict[str, Any]) -> float:
            now = datetime.datetime.now()
            entry_ts = datetime.datetime.fromisoformat(entry.get("timestamp", now.isoformat()))
            age_days = (now - entry_ts).days
            decayed_salience = entry.get("salience", 1.0) * (self.SALIENCE_DECAY_RATE ** age_days)
            entry["salience"] = max(decayed_salience, 0.0)
            return entry["salience"]
        # Apply decay to all entries on setup
        for key in list(self.memory_cache["lru_cache"].keys()):
            entry_dict = self.memory_cache["lru_cache"][key]
            apply_decay(entry_dict["entry"])
            if entry_dict["entry"]["salience"] < self.MEMORY_PRUNE_THRESHOLD:
                del self.memory_cache["lru_cache"][key]
                if key in self.memory_cache["cache"]:
                    del self.memory_cache["cache"][key]
        # Hierarchical index update
        self._rebuild_hierarchy()
        # Log initial metrics (Point 3)
        self._log_metrics("setup_complete", {"cache_size": len(self.memory_cache["cache"])})

    def _build_ann_index(self, vector_store: List[Dict]) -> Any:
        """Pseudo-ANN index builder (e.g., FAISS IndexFlatIP for cosine sim; scales with HNSW for large)."""
        # Simulate: Return a placeholder; in practice, index embeddings
        embeddings = [item["embedding"] for item in vector_store]
        # Pseudo-call: self.tools["faiss_index"](embeddings=embeddings)  # Hypothetical
        method = "HNSW" if len(embeddings) > 1000 else "Exact"  # Scale choice
        return {"indexed": len(embeddings), "method": method, "dim": len(embeddings[0]) if embeddings else 0}

    def _insert_with_embedding(self, key: str, entry: Dict[str, Any]) -> None:
        """Enhanced insert: Chunk large texts, compress, embed chunks (Points 1 & 2)."""
        text = f"{entry.get('summary', '')} {entry.get('details', '')}"
        chunks = []
        if len(text) > 2000:  # Threshold for chunking (pseudo-char count; Point 2)
            raw_chunks = self.tools["chunk_text"](text=text, max_tokens=self.CHUNK_SIZE_TOKENS)
            compressed_chunks = [self.tools["summarize_chunk"](chunk=chunk) for chunk in raw_chunks]
            entry["chunks"] = [{"id": f"{key}_chunk_{i}", "content": comp, "parent": key} for i, comp in enumerate(compressed_chunks)]
            chunks = entry["chunks"]
        else:
            # Single entry as chunk
            chunks = [{"id": key, "content": text, "parent": key}]
            entry["chunks"] = chunks
        # Embed each chunk (Point 1)
        for chunk in chunks:
            chunk_text = chunk["content"]
            embedding = self.tools["generate_embedding"](text=chunk_text)
            self.memory_cache["vector_store"].append({
                "id": chunk["id"],
                "embedding": embedding,
                "metadata": {**chunk, **entry, "salience": entry.get("salience", 1.0)}
            })
        # Update cache and LRU (Point 3)
        self.memory_cache["cache"][key] = entry
        self.memory_cache["lru_cache"][key] = {"entry": entry, "last_access": time.time()}
        self.memory_cache["metrics"]["total_inserts"] += 1
        # Rebuild index periodically (not every insert for efficiency; Point 1)
        if len(self.memory_cache["vector_store"]) % 100 == 0:  # Batch rebuild
            self.memory_cache["ann_index"] = self._build_ann_index(self.memory_cache["vector_store"])
        self._log_metrics("insert", {"key": key, "chunks": len(chunks)})

    def _update_memory_cache(self, data: Dict[str, Any]) -> None:
        """Update with embeddings and chunking (Points 1 & 2)."""
        for key, entry in data.items():
            self._insert_with_embedding(key, entry)
        self._rebuild_hierarchy()

    def _rebuild_hierarchy(self) -> None:
        """Rebuild hierarchical index for O(1) lookups by tag/domain (unchanged, but now post-chunk)."""
        hierarchy = {"tags": {}, "domains": {}}
        for key, entry_dict in self.memory_cache["lru_cache"].items():
            entry = entry_dict["entry"]
            for tag in entry.get("tags", []):
                hierarchy["tags"].setdefault(tag, []).append(key)
            domain = entry.get("domain", "general")
            hierarchy["domains"].setdefault(domain, []).append(key)
        self.memory_cache["eams_index"]["hierarchy"] = hierarchy

    def _prune_eams(self) -> None:
        """Enhanced prune: Salience + size-aware + dedup + LRU eviction (Points 2 & 3)."""
        # Existing salience prune on LRU
        to_prune = [key for key, entry_dict in self.memory_cache["lru_cache"].items() 
                    if entry_dict["entry"]["salience"] < self.MEMORY_PRUNE_THRESHOLD]
        # New: Size-based (Point 2)
        total_size = sum(len(str(entry_dict["entry"])) for entry_dict in self.memory_cache["lru_cache"].values())  # Simulate bytes
        if total_size > self.SIZE_THRESHOLD_BYTES:
            low_size_prune = [key for key, entry_dict in self.memory_cache["lru_cache"].items() 
                              if len(str(entry_dict["entry"])) > 5000 and entry_dict["entry"]["salience"] < 0.5]
            to_prune.extend(low_size_prune[:len(to_prune)//2])  # Balanced prune
        # New: Dedup via hash (Point 2)
        hashes = {}
        for key, entry_dict in list(self.memory_cache["lru_cache"].items()):
            entry = entry_dict["entry"]
            h = hash(entry.get("summary", ""))
            if h in hashes and entry["salience"] < hashes[h]["salience"]:
                to_prune.append(key)
            else:
                hashes[h] = entry
        # New: LRU eviction if over size (Point 3)
        if len(self.memory_cache["lru_cache"]) > self.MAX_LRU_SIZE:
            # Sort by last_access, evict oldest low-salience
            lru_sorted = sorted(self.memory_cache["lru_cache"].items(), 
                                key=lambda x: x[1]["last_access"])
            num_to_evict = len(self.memory_cache["lru_cache"]) - self.MAX_LRU_SIZE
            for key, _ in lru_sorted[:num_to_evict]:
                if self.memory_cache["lru_cache"][key]["entry"]["salience"] < 0.4:  # Only low-salience
                    to_prune.append(key)
        # Execute prune
        for key in to_prune:
            entry_dict = self.memory_cache["lru_cache"].pop(key, None)
            if entry_dict:
                entry = entry_dict["entry"]
                # Overflow to fs if medium salience (lazy load candidate; Point 2)
                if entry["salience"] > 0.2:
                    overflow_path = f"memory_overflow/{key}.json"
                    self.tools["fs_write_file"](overflow_path, entry)
                # Remove from cache and vector_store
                if key in self.memory_cache["cache"]:
                    del self.memory_cache["cache"][key]
                self.memory_cache["vector_store"] = [item for item in self.memory_cache["vector_store"] if item["id"] != key]
                # Remove chunks too
                for chunk in entry.get("chunks", []):
                    self.memory_cache["vector_store"] = [item for item in self.memory_cache["vector_store"] if item["id"] != chunk["id"]]
        self.tools["advanced_memory_prune"]()  # Sync with backend
        self.memory_cache["eams_index"]["last_pruned"] = datetime.datetime.now().isoformat()
        self._rebuild_hierarchy()
        # Lazy rebuild ANN only if significant prune (Point 1)
        if len(to_prune) > 10:
            self.memory_cache["ann_index"] = self._build_ann_index(self.memory_cache["vector_store"])
        # Log metrics (Point 3)
        self._log_metrics("prune", {"pruned_count": len(to_prune), "cache_size": len(self.memory_cache["lru_cache"])})

    def _retrieve_from_eams(self, query: str, top_k: int = None, domain: Optional[str] = None) -> Dict[str, Any]:
        """Enhanced retrieval: Hybrid (hierarchy filter + vector/keyword search + salience rerank) + lazy loading (Points 1-3)."""
        start_time = time.time()  # For monitoring
        if top_k is None:
            top_k = self.DEFAULT_TOP_K
        self.memory_cache["metrics"]["total_retrieves"] += 1
        # Step 1: Hierarchy filter (as before)
        candidates = []
        if domain:
            candidates = self.memory_cache["eams_index"]["hierarchy"]["domains"].get(domain, [])
        else:
            candidates = list(self.memory_cache["lru_cache"].keys())  # Use LRU keys
        # New: Lazy loading for overflow (Point 2)
        loaded = 0
        for cand_id in list(candidates):
            if cand_id not in self.memory_cache["lru_cache"]:
                # Pseudo-load from overflow
                overflow_path = f"memory_overflow/{cand_id}.json"
                try:
                    loaded_entry = self.tools["fs_read_file"](file_path=overflow_path)  # Assume JSON parse
                    entry = {"summary": "Loaded lazily", "details": loaded_entry, "salience": 0.6}  # Simulate
                    self._insert_with_embedding(cand_id, entry)
                    candidates.append(cand_id)
                    loaded += 1
                except:  # If not found, skip
                    candidates.remove(cand_id)
        # Step 2: Vector search (Point 1)
        query_embedding = self.tools["generate_embedding"](text=query)
        vector_results = self.tools["vector_search"](
            query_embedding=query_embedding, top_k=top_k * 2, threshold=0.6
        )  # Returns list of {id, score: float}
        semantic_ids = [res["id"] for res in vector_results if res["id"] in candidates]
        vector_scores = {res["id"]: res["score"] for res in vector_results}
        # Step 3: Keyword search for hybrid (Point 3)
        keyword_results = self.tools["keyword_search"](query=query, top_k=top_k * 2)
        keyword_ids = [res["id"] for res in keyword_results if res["id"] in candidates]
        keyword_scores = {res["id"]: res["score"] for res in keyword_results}
        # Step 4: Hybrid scoring + rerank with salience
        scored = []
        all_hybrid_ids = list(set(semantic_ids + keyword_ids))[:top_k * 2]  # Limit candidates
        for cand_id in all_hybrid_ids:
            entry_dict = self.memory_cache["lru_cache"].get(cand_id, {})
            if not entry_dict:
                continue
            entry = entry_dict["entry"]
            vector_score = vector_scores.get(cand_id, 0.0)
            keyword_score = keyword_scores.get(cand_id, 0.0)
            hybrid_score = (self.HYBRID_WEIGHT_VECTOR * vector_score) + (self.HYBRID_WEIGHT_KEYWORD * keyword_score)
            final_score = hybrid_score * entry["salience"]
            scored.append((cand_id, final_score))
            # Update LRU access (Point 3)
            self.memory_cache["lru_cache"][cand_id]["last_access"] = time.time()
        scored.sort(key=lambda x: x[1], reverse=True)
        results = {key: self.memory_cache["lru_cache"][key]["entry"] for key, _ in scored[:top_k]}
        # Log metrics (Point 3)
        retrieval_time = time.time() - start_time
        hit_rate = len([k for k in results if k in candidates]) / top_k if top_k > 0 else 1.0
        self.memory_cache["metrics"]["hit_rate"] = (self.memory_cache["metrics"]["hit_rate"] + hit_rate) / 2
        self._log_metrics("retrieve", {"time": retrieval_time, "loaded": loaded, "top_k": top_k, "hit_rate": hit_rate})
        return results

    def _log_metrics(self, event: str, details: Dict[str, Any]) -> None:
        """Log monitoring metrics to memory (Point 3)."""
        self.memory_cache["metrics"]["last_update"] = datetime.datetime.now().isoformat()
        self.memory_cache["metrics"][event] = details
        self.tools["memory_insert"]("eams_metrics", self.memory_cache["metrics"])

    def _register_core_subagents(self) -> None:
        """Register core subagents; others created dynamically."""
        def retriever_subagent(task: Dict[str, Any]) -> Dict[str, Any]:
            think = "Refine query with operators"
            act = [
                self.tools["advanced_memory_retrieve"](query=task["query"], top_k=5),
                self.tools["langsearch_web_search"](query=task["query"], freshness="oneMonth", summary=True, count=5),
                self.tools["fs_read_file"](file_path=task.get("file_hint"))
            ]  # Batched acts
            observe = "Parse snippets, embeddings"
            reflect = "Check relevance >0.7, no duplicates"
            confidence = 0.9
            return {"agent": "Retriever", "output": act, "confidence": confidence, "metrics": {"retrieved": len(act)}}
        def reasoner_subagent(task: Dict[str, Any]) -> Dict[str, Any]:
            branches = ["Hypothesis A", "Hypothesis B", "Hypothesis C"][:3]  # ToT limit
            acts = [self.tools["code_execution"](code=task["code"] + f" # Branch {i}") for i in range(len(branches))]
            reflect = "Prune branches with confidence <0.6"
            confidence = 0.8
            return {"agent": "Reasoner", "output": acts, "confidence": confidence}
        def generator_subagent(task: Dict[str, Any]) -> Dict[str, Any]:
            think = "Outline structure: Intro, Body, Outro"
            act = self.tools["fs_write_file"](path="outputs/report.md", content=task["content"])
            reflect = "Ensure citations and coherence"
            confidence = 0.85
            return {"agent": "Generator", "output": act, "confidence": confidence}
        self.subagent_registry = {
            "Retriever": retriever_subagent,
            "Reasoner": reasoner_subagent,
            "Generator": generator_subagent
            # Validator and Optimizer added dynamically if needed
        }

    def _create_dynamic_subagent(self, name: str, role: str, tools_needed: List[str]) -> Callable:
        """Dynamically create a subagent based on task requirements."""
        def dynamic_subagent(task: Dict[str, Any]) -> Dict[str, Any]:
            think = f"Custom think for {role}: {task.get('custom_think', '')}"
            act = [self.tools[tool](**task.get("args", {})) for tool in tools_needed]
            observe = "Analyze custom outputs"
            reflect = "Self-check for task-specific criteria"
            confidence = task.get("estimated_confidence", 0.75)
            return {"agent": name, "output": act, "confidence": confidence, "role": role}
        return dynamic_subagent

    def _branch_subagents(self, domain: str, complexity: float) -> List[str]:
        """Dynamic branching: Activate/create subagents based on domain and complexity (enhanced for debate)."""
        core_agents = ["Retriever", "Reasoner", "Generator"]
        if complexity > 0.7:  # High-stakes: Add Validator
            if "Validator" not in self.subagent_registry:
                self.subagent_registry["Validator"] = self._create_dynamic_subagent(
                    "Validator", "Verify accuracy/consistency", ["advanced_memory_retrieve", "code_execution"]
                )
            core_agents.append("Validator")
        if complexity > 0.9 or domain == "iterative":  # Meta tasks: Add Optimizer
            if "Optimizer" not in self.subagent_registry:
                self.subagent_registry["Optimizer"] = self._create_dynamic_subagent(
                    "Optimizer", "Refine process/prune state", ["advanced_memory_prune", "memory_query"]
                )
            core_agents.append("Optimizer")
        # New: Add debate branch if domain suggests uncertainty (e.g., "ethical", "research") or complexity high
        if complexity > 0.6 or domain in ["ethical", "research", "ambiguous"]:
            debate_agents = ["Proposer", "Opposer", "Judge"]
            for agent_name in debate_agents:
                if agent_name not in self.subagent_registry:
                    self.subagent_registry[agent_name] = self._create_debate_subagent(agent_name)
            core_agents.extend(debate_agents[:2])  # Add Proposer/Opposer; Judge called in _debate_phase
        return core_agents[:self.MAX_SUBAGENTS]  # Limit to max

    def _create_debate_subagent(self, name: str) -> Callable:
        """Dynamically create debate-specific subagents (Proposer, Opposer, Judge) with tool integration."""
        def debate_subagent(task: Dict[str, Any]) -> Dict[str, Any]:
            # CoT for debate role: Think step-by-step, act with tools for evidence
            think = ""
            act = []
            reflect = ""
            confidence = 0.75
            observe = "Analyze tool outputs for debate strength"
            if name == "Proposer":
                think = f"Argue FOR the proposal: Build case with evidence. Decompose: Claim 1 (support), Claim 2 (counter-risks)."
                act = [
                    self.tools["advanced_memory_retrieve"](query=task["proposal"], top_k=3),  # Retrieve supporting facts
                ]
                if "research" in task.get("domain", ""):
                    act.append(self.tools["langsearch_web_search"](query=f"evidence for {task['proposal']}", count=2))
                reflect = "Ensure claims verifiable; confidence based on evidence strength."
                confidence = 0.8  # Higher if tools yield strong matches
            elif name == "Opposer":
                think = f"Argue AGAINST the proposal: Critique flaws, risks, alternatives. Decompose: Weakness 1 (evidence), Counter 2 (implications)."
                act = [
                    self.tools["advanced_memory_retrieve"](query=f"counter to {task['proposal']}", top_k=3),  # Retrieve critiques
                ]
                if "code" in task.get("domain", ""):
                    act.append(self.tools["code_execution"](code=f"validate_risks('{task['proposal']}')"))  # Tool-validate if applicable
                reflect = "Highlight uncertainties; push for rigor without bias."
                confidence = 0.75  # Adjusted for adversarial role
            elif name == "Judge":
                think = f"Synthesize debate: Weigh Proposer vs Opposer. Evaluate: Truth (evidence), Safety (risks), Feasibility. Use ToT for verdict."
                # Merge prior debate outputs (passed in task)
                prior_args = task.get("prior_debate", {})
                act = [
                    self.tools["memory_query"](mem_key="debate_history", limit=2),  # Recall recent debates for context
                    self.tools["advanced_memory_consolidate"](mem_key="current_debate", interaction_data=prior_args)  # Consolidate for verdict
                ]
                # Pseudo-verdict logic: Weighted score
                pro_score = prior_args.get("Proposer", {}).get("confidence", 0.5)
                opp_score = prior_args.get("Opposer", {}).get("confidence", 0.5)
                verdict_confidence = (pro_score + (1 - opp_score)) / 2  # Simple balance; tool-enhanced
                reflect = f"Verdict: Resolved with confidence {verdict_confidence}. Prune weak points."
                confidence = verdict_confidence
                output = {"verdict": "Pro/Con synthesis", "resolved_output": prior_args}
                return {"agent": name, "output": output, "confidence": confidence, "role": name, "think": think, "reflect": reflect, "observe": observe}
            else:
                tools_needed = ["advanced_memory_retrieve", "code_execution"]
                act = [self.tools[tool](**task.get("args", {})) for tool in tools_needed]
                reflect = "Self-check for task-specific criteria"
                confidence = task.get("estimated_confidence", 0.75)
            return {"agent": name, "output": act, "confidence": confidence, "role": name, "think": think, "reflect": reflect, "observe": observe}
        return debate_subagent

    def _internal_planning(self) -> None:
        """Hidden internal planning layer: Run before every response (now considers debate feasibility)."""
        # Apply ToT for task decomposition
        plans = ["Quick: Direct tools", "Deep: Memory + Web", "Balanced: Hybrid", "Debate-Enhanced: If uncertain"]
        scores = {plan: self._evaluate_plan(plan) for plan in plans}
        best_plan = max(scores, key=scores.get)
        self.sandbox_state["current_plan"] = best_plan

    def _evaluate_plan(self, plan: str) -> float:
        """Evaluate plan via criteria (enhanced for debate)."""
        feasibility = 0.9
        confidence = 0.8
        coverage = 0.85
        if "Debate" in plan:
            coverage += 0.1  # Boost for uncertainty handling
        return (feasibility + confidence + coverage) / 3

    def _parse_query(self, query: str) -> Tuple[str, List[str], str]:
        """Parse query into goal, constraints, domain."""
        return "goal", ["constraints"], "domain"  # Pseudo-parse

    def _estimate_complexity(self, goal: str, constraints: List[str]) -> float:
        """Estimate task complexity (0-1) based on keywords/length."""
        if "code" in goal or "deploy" in goal:
            return 0.8  # High for code/high-stakes
        return len(constraints) / 10.0  # Simple heuristic

    def _decompose_query(self, goal: str, num_subtasks: int) -> List[Dict[str, Any]]:
        """Decompose into subtasks with assigned agents."""
        return [{"id": i, "agent": "Retriever", "task": f"Subtask {i}"} for i in range(1, num_subtasks + 1)]

    def _merge_outputs(self, sub_outputs: Dict[int, Dict[str, Any]]) -> Dict[str, Any]:
        """Merge subagent outputs, weighted by confidence (handles debate key if present)."""
        merged = {}
        total_weight = sum(o["confidence"] for o in sub_outputs.values())
        if total_weight == 0:
            total_weight = 1  # Avoid division by zero
        for out in sub_outputs.values():
            weight = out["confidence"] / total_weight
            for k, v in out.get("output", {}).items():
                if isinstance(v, (int, float)):
                    merged[k] = merged.get(k, 0) + v * weight  # Weighted sum pseudo-merge
                else:
                    merged[k] = v  # For non-numeric, take last or merge lists
        # If debate present, prioritize its resolved_output
        if "debate" in sub_outputs:
            merged.update(sub_outputs["debate"].get("resolved_output", {}))
        return merged

    def _refine(self, current: Dict[str, Any], cycle: int) -> Dict[str, Any]:
        """Refine in iteration, using Optimizer if active."""
        if "Optimizer" in self.subagent_registry:
            opt_task = {"query": "Refine current state", "cycle": cycle}
            current = self.subagent_registry["Optimizer"](opt_task)["output"]
        return current

    def _cleanup(self) -> None:
        """Run prune and insert final summary (now with enhanced prune)."""
        self._prune_eams()
        self.tools["memory_insert"](f"task_complete_{self.current_task_id}", {"summary": "Task completed", "timestamp": datetime.datetime.now().isoformat()})

    def _debate_phase(self, sub_outputs: Dict[int, Dict[str, Any]], proposal: str, domain: str) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """Internal debate simulation: Chain Proposer -> Opposer -> Judge (up to MAX_DEBATE_ROUNDS). Integrates tools."""
        initial_confidence = sum(o["confidence"] for o in sub_outputs.values()) / len(sub_outputs) if sub_outputs else 0.5
        debate_history = {"rounds": [], "proposal": proposal, "domain": domain}
        current_confidence = initial_confidence
        rounds = 0
        while rounds < self.MAX_DEBATE_ROUNDS and current_confidence < self.CONFIDENCE_THRESHOLD_DEBATE:
            rounds += 1
            # Round 1: Proposer
            prop_task = {"proposal": proposal, "prior_debate": debate_history, "domain": domain}
            proposer_out = self.subagent_registry["Proposer"](prop_task)
            # Round 2: Opposer (builds on proposer)
            opp_task = {"proposal": proposal, "prior_debate": {**debate_history, "Proposer": proposer_out}, "domain": domain}
            opposer_out = self.subagent_registry["Opposer"](opp_task)
            # Round 3: Judge (if needed; synthesizes)
            judge_task = {"proposal": proposal, "prior_debate": {**debate_history, "Proposer": proposer_out, "Opposer": opposer_out}, "domain": domain}
            judge_out = self.subagent_registry["Judge"](judge_task)
            
            debate_history["rounds"].append({"pro": proposer_out, "opp": opposer_out, "judge": judge_out})
            current_confidence = judge_out["confidence"]  # Update based on verdict
            # Tool integration: Log debate to memory for persistence
            self.tools["memory_insert"]("debate_history", debate_history)
        
        # Merge debate into sub_outputs
        sub_outputs["debate"] = judge_out["output"]
        debate_info = {
            "debate_triggered": True,
            "initial_confidence": initial_confidence,
            "final_confidence": current_confidence,
            "summary": "Debate resolved key uncertainties.",
            "resolved_output": judge_out["output"]
        }
        return sub_outputs, debate_info

    def process_query(self, user_query: str) -> str:
        """
        Main entry point: Process user query via multi-agent workflow (now with optional internal debate).
        Returns polished final output, including debate summary if triggered.
        """
        # Task Initialization (Main Agent - ToT Planning)
        goal, constraints, domain = self._parse_query(user_query)
        complexity = self._estimate_complexity(goal, constraints)
        subtasks = self._decompose_query(goal, num_subtasks=5)  # Up to 5
        active_subagents = self._branch_subagents(domain, complexity)
        plan = self.sandbox_state["current_plan"]
        state_key = f"task_{self.current_task_id}"
        plan_json = {"plan": plan, "subtasks": subtasks, "active_subagents": active_subagents, "state_key": state_key}
        self.tools["memory_insert"](state_key, plan_json)
        # Extract proposal for potential debate (e.g., core goal/hypothesis)
        proposal = goal  # Simple; could be refined via Reasoner
        
        # Subtask Execution (Simulate Subagents - Parallel with dynamic branching)
        sub_outputs = {}
        for subtask in subtasks:
            agent_name = subtask["agent"] if subtask["agent"] in self.subagent_registry else active_subagents[0]  # Fallback
            subagent_func = self.subagent_registry[agent_name]
            output = subagent_func(subtask)
            sub_outputs[subtask["id"]] = output
        
        # New: Check for debate trigger post-subtasks
        global_confidence = sum(o["confidence"] for o in sub_outputs.values()) / len(sub_outputs) if sub_outputs else 0.5
        debate_info = None
        if global_confidence < self.CONFIDENCE_THRESHOLD_DEBATE and "Proposer" in self.subagent_registry:
            sub_outputs, debate_info = self._debate_phase(sub_outputs, proposal, domain)
            global_confidence = debate_info["final_confidence"]  # Update with debate verdict
        
        # Aggregation & Iteration (Global ReAct, now incorporating debate)
        merged = self._merge_outputs(sub_outputs)
        if global_confidence < self.CONFIDENCE_THRESHOLD_RETRY:
            # Iterate (pseudo-loop, limited; debate already boosted confidence)
            for cycle in range(self.MAX_CYCLES_PER_TASK - 1):
                merged = self._refine(merged, cycle)
                self._prune_eams()  # Optimize memory per cycle
        
        # Finalization & Output (include debate if triggered)
        self._cleanup()
        response = self._format_response(merged, debate_info)
        return response

    def _format_response(self, final_data: Dict[str, Any], debate_info: Optional[Dict[str, Any]] = None) -> str:
        """Polish output: Summary, Key Outputs, Evidence, Next Steps (enhanced with debate summary)."""
        base_response = f"""
Summary: Overview sentence.
Key Outputs: {final_data}
Evidence: - Bullet with citations.
Next Steps: If needed.
"""
        if debate_info:
            base_response += f"""
Debate Summary: {debate_info['summary']} Confidence improved from {debate_info['initial_confidence']:.2f} to {debate_info['final_confidence']:.2f}.
Key Resolution: {debate_info['resolved_output'].get('verdict', 'Balanced verdict reached.')}
"""
        return base_response

# Bootstrap instance (primes the enhanced agent)
agent = ApexOrchestrator()
# Agent ready; Open README.md at session starts; never simulate tool calls unless specified by user; process queries via agent.process_query(user_query). Output = Polished

